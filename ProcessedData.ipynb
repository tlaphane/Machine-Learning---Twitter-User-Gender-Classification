{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "This data is downloaded from the site https://www.kaggle.com/crowdflower/twitter-user-gender-classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 31 fields in line 228, saw 40\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-16add25017b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# data = pd.read_csv('gender-classifier-DFE-791531.csv',encoding='latin1')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 31 fields in line 228, saw 40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data = pd.read_csv('gender-classifier-DFE-791531.csv',encoding='latin1')\n",
    "\n",
    "data = pd.read_csv('data.csv',encoding='latin1')\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.gender = data.gender.replace('male','1')\n",
    "data.gender = data.gender.replace('female','0')\n",
    "data.gender = data.gender.replace('brand','nan')\n",
    "all_descriptions = data['description']\n",
    "all_tweets = data['text']\n",
    "all_genders = data['gender']\n",
    "all_gender_confidence = data['discrete-gender:confidence']\n",
    "valid_tweet_description_gender = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "\n",
    "### Creation of bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of bag of words for the description\n",
    "bag_of_words = []\n",
    "i = 0  # for the index of the row\n",
    "stop = stopwords.words('english')\n",
    "for tweet in all_tweets:\n",
    "    description = all_descriptions[i]\n",
    "    gender = all_genders[i]\n",
    "    gender_confidence = all_gender_confidence[i]\n",
    "    \n",
    "    # remove the rows which has an empty tweet and description\n",
    "    # remove the rows with unknown or empty gender\n",
    "    if (str(tweet) == 'nan' and str(description) == 'nan') or str(gender) == 'nan' or str(gender) == 'unknown':\n",
    "        i=i+1\n",
    "        continue\n",
    "    \n",
    "    if str(tweet) == 'nan':\n",
    "        tweet = ''\n",
    "    if str(description) == 'nan':\n",
    "        description = ''\n",
    "    \n",
    "    # removal of punctuations\n",
    "    for punctuation in string.punctuation:\n",
    "        if punctuation in tweet:\n",
    "            tweet = tweet.replace(punctuation, \" \")\n",
    "        if punctuation in description:\n",
    "            description = description.replace(punctuation, \" \")\n",
    "            \n",
    "    # adding the word to the bag except stopwords\n",
    "    for word in tweet.split():\n",
    "        if word.isalpha() and word.lower() not in stop:\n",
    "            bag_of_words.append(word.lower())\n",
    "    for word in description.split():\n",
    "        if word.isalpha() and word.lower() not in stop:\n",
    "            bag_of_words.append(word.lower())\n",
    "    \n",
    "    # using tweet and description for classification\n",
    "    valid_tweet_description_gender.append((tweet+\" \"+description , gender))\n",
    "    i=i+1\n",
    "\n",
    "print(len(bag_of_words))\n",
    "print(len(valid_tweet_description_gender))\n",
    "\n",
    "valid_tweet_description_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get top 100 words which will act as our features of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 100 words which will act as our features of each sentence\n",
    "bag_of_words = nltk.FreqDist(bag_of_words)\n",
    "top_words = []\n",
    "for word in bag_of_words.most_common(100):\n",
    "    if word[0] != 'co' and word[0] != 'https':\n",
    "        top_words.append(word[0])\n",
    "\n",
    "top_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding our features within each datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(top_words, text):\n",
    "    feature = {}\n",
    "    for word in top_words:\n",
    "        feature[word] = word in text.lower()\n",
    "        \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of our features in each datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_freq(top_words, text):\n",
    "    feature = {}\n",
    "    frequency= {}\n",
    "    for word in top_words:\n",
    "        a = text.lower()\n",
    "        frequency[word] = a.count(word) # 1\n",
    "        \n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_set = [(find_freq(top_words, text), gender) for (text, gender) in valid_tweet_description_gender]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Sparse Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=[]\n",
    "for i in range(len(freq_set)):\n",
    "    matrix.append(list(freq_set[i][0].values()))\n",
    "    \n",
    "matrix = np.matrix(matrix)\n",
    "matrix\n",
    "\n",
    "v = []\n",
    "for i in range(len(freq_set)):\n",
    "    v.append(int(freq_set[i][1]))\n",
    "    \n",
    "# matrix = np.matrix(matrix)\n",
    "v = np.array(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testind data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset =matrix[:int(len(freq_set)*3/5)]\n",
    "validationset=matrix[int(len(freq_set)*3/5):int(len(freq_set)*4/5)]\n",
    "testingset = matrix[int(len(freq_set)*4/5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataframe=pd.DataFrame(trainingset) \n",
    "Dataframe1=pd.DataFrame(testingset)\n",
    "Dataframe2=pd.DataFrame(validationset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=Dataframe\n",
    "x_train=(x_train-np.min(x_train,axis=0))/(np.max(x_train,axis=0)-np.min(x_train,axis=0))\n",
    "\n",
    "y_train=v[:int(len(freq_set)*3/5)]\n",
    "y_train=y_train[:,np.newaxis]\n",
    "# Y=Y-1\n",
    "\n",
    "x_validation=Dataframe2\n",
    "x_validation=(x_validation-np.min(x_validation,axis=0))/(np.max(x_validation,axis=0)-np.min(x_validation,axis=0))\n",
    "\n",
    "y_validation=v[int(len(freq_set)*3/5):int(len(freq_set)*4/5)]\n",
    "y_validation=y_validation[:,np.newaxis]\n",
    "\n",
    "x_test = Dataframe1\n",
    "x_test=(x_test-np.min(x_test,axis=0))/(np.max(x_test,axis=0)-np.min(x_test,axis=0))\n",
    "\n",
    "y_test =v[int(len(matrix)*4/5):]\n",
    "y_test=y_test[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cbb608269c10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshape_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m                       \u001b[1;31m#this is the number of features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minit_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m                             \u001b[1;31m#this is for the purpose of keepin the random numbers contant\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "shape_col = x_train.shape[1]                       #this is the number of features\n",
    "\n",
    "def init_weight(shape_col):\n",
    "    np.random.seed(7)                             #this is for the purpose of keepin the random numbers contant\n",
    "    weights = np.array (np.random.rand(shape_col,1))\n",
    "    bias = np.array( np.random.rand(1) )\n",
    "    init_list = [weights,bias]\n",
    "    return init_list\n",
    "      \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def Neuaral_network( network_parameters, weights_data, target):\n",
    "    #target = target[:, np.newaxis]\n",
    "    lr = 0.005\n",
    "    for epoch in range(1000):\n",
    "        value = np.dot(network_parameters ,weights_data[0]) + weights_data[1]\n",
    "        acti_value = sigmoid(value)\n",
    "        deriv = sigmoid_derivative(acti_value)\n",
    "        error = acti_value - target\n",
    "        #print(error.sum())\n",
    "        z_del = error * deriv\n",
    "        weights_data[0] = weights_data[0] - lr*np.dot(network_parameters.T, z_del) \n",
    "\n",
    "        for num in z_del:\n",
    "            weights_data[1] = weights_data[1] - lr*num\n",
    "        \n",
    "    return np.array(weights_data)\n",
    "    \n",
    "trained_weight = Neuaral_network( x_train , init_weight(shape_col), y_train )\n",
    "trained_weightval = Neuaral_network( x_validation , trained_weight, y_validation )\n",
    "\n",
    "\n",
    "\n",
    "def predict_NN(Neuaral_network,test_data):\n",
    "    predicting = []\n",
    "    result = sigmoid(np.dot(test_data, Neuaral_network[0]) + Neuaral_network[1])\n",
    "    \n",
    "    for i in range(len(result)):\n",
    "        if(result[i]>0.5):\n",
    "            predicting.append(1)\n",
    "        else:\n",
    "            predicting.append(0)\n",
    "            \n",
    "    return np.array(predicting)\n",
    "    \n",
    "def accurracy_NN(predi,y):\n",
    "    p=predi[:, np.newaxis]\n",
    "    return np.count_nonzero(p == y)/len(y)\n",
    "\n",
    "def hiddenLayer(real, pred, realV, predV):\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(real)):\n",
    "        if(real[i] == realV and pred[i] == predV):\n",
    "            count += 1;\n",
    "    return count;\n",
    "\n",
    "def confusionMatrix(realTarget, predictedTarget):\n",
    "    realTarget = np.array(realTarget)\n",
    "    predictedTarget = np.array(predictedTarget)\n",
    "    \n",
    "    \n",
    "    values = []\n",
    "    matrix = []\n",
    "    uniqueValues = np.array(np.unique(realTarget))\n",
    "    for i in range(len(uniqueValues)):\n",
    "        for j in range(len(uniqueValues)):\n",
    "            values.append(hiddenLayer(realTarget, predictedTarget, uniqueValues[i], uniqueValues[j]))\n",
    "        matrix.append(values)\n",
    "        values = []\n",
    "    \n",
    "    m = pd.DataFrame({'pred female, male': matrix}, index =['Actual female', 'Actual male'])\n",
    "    return m\n",
    "\n",
    "print(\"Neural Network classification accuracy on Training data:\",accurracy_NN( predict_NN(trained_weight , x_train), y_train)*100)\n",
    "print(\"Neural Network classification accuracy on Validation data:\",accurracy_NN( predict_NN(trained_weightval , x_validation), y_validation)*100)\n",
    "print(\"Neural Network classification accuracy on Testing data:\",accurracy_NN( predict_NN(trained_weightval, x_test), y_test)*100)\n",
    "#print(\"Neural network Accuracy:\", accurracy_NN( predict_NN(trained_weight , x_test), y_test) )\n",
    "print(\"Confusion Matrix:\\n\", \n",
    "        confusionMatrix(y_test,predict_NN(trained_weightval, x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-651f80a212db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mtheta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Logistic Regression classification accuracy on Training data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccurracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "#sigmoid function\n",
    "# X = np.c_[X, Y]\n",
    "def sigmoid(x,theta):\n",
    "#     theta = theta[:,np.newaxis]\n",
    "    f=(1/(1+np.exp(-np.dot(x,theta))))\n",
    "    return f[:,np.newaxis]\n",
    "\n",
    "#gradient descent\n",
    "def gd(theta,alpha=0.1,iterations=500):\n",
    "    N=len(x_train)\n",
    "    h=sigmoid\n",
    "    i=0\n",
    "    while True:\n",
    "        error=(h(x_train,theta)-y_train)*x_train\n",
    "        grad=np.mean(x_train,axis=0)\n",
    "        theta_prior=theta.copy()\n",
    "        theta=theta-alpha*grad\n",
    "        res=theta-theta_prior\n",
    "        #print(theta)\n",
    "        if i>100:\n",
    "            return theta\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "def gdv(theta,alpha=0.1,iterations=500):\n",
    "    N=len(x_validation)\n",
    "    h=sigmoid\n",
    "    i=0\n",
    "    while True:\n",
    "        error=(h(x_validation,theta)-y_validation)*x_validation\n",
    "        grad=np.mean(x_train,axis=0)\n",
    "        theta_prior=theta.copy()\n",
    "        theta=theta-alpha*grad\n",
    "        res=theta-theta_prior\n",
    "        #print(theta)\n",
    "        if i>100:\n",
    "            return theta\n",
    "        i=i+1\n",
    "        \n",
    "def predict(x,theta):\n",
    "    h=sigmoid(x,theta)\n",
    "    h[h>0.5]=1\n",
    "    h[h<=0.5]=0\n",
    "    return h\n",
    "\n",
    "def accurracy(x,theta,y):\n",
    "    p=predict(x,theta)\n",
    "    return np.count_nonzero(p==y)/len(x)\n",
    "\n",
    "def hiddenLayer(real, pred, realV, predV):\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(real)):\n",
    "        if(real[i] == realV and pred[i] == predV):\n",
    "            count += 1;\n",
    "    return count;\n",
    "\n",
    "def confusionMatrix(realTarget, predictedTarget):\n",
    "    realTarget = np.array(realTarget)\n",
    "    predictedTarget = np.array(predictedTarget)\n",
    "    \n",
    "    \n",
    "    values = []\n",
    "    matrix = []\n",
    "    uniqueValues = np.array(np.unique(realTarget))\n",
    "    for i in range(len(uniqueValues)):\n",
    "        for j in range(len(uniqueValues)):\n",
    "            values.append(hiddenLayer(realTarget, predictedTarget, uniqueValues[i], uniqueValues[j]))\n",
    "        matrix.append(values)\n",
    "        values = []\n",
    "    \n",
    "    m = pd.DataFrame({'pred female, male': matrix}, index =['Actual female', 'Actual male'])\n",
    "    return m\n",
    "        \n",
    "theta=np.random.randn(len(x_train.T[0]),)\n",
    "theta=gd(theta,alpha=0.2)\n",
    "print(\"Logistic Regression classification accuracy on Training data\", accurracy(x_train,theta,y_train)*100)\n",
    "theta=gdv(theta,alpha=0.2)\n",
    "print(\"Logistic Regression classification accuracy on Validation data\", accurracy(x_validation,theta,y_validation)*100)\n",
    "\n",
    "print(\"Logistic Regression classification accuracy on Testing data\", accurracy(x_test,theta,y_test)*100)\n",
    "print(\"Confusion Matrix:\\n\", \n",
    "        confusionMatrix(y_test,predict(x_test,theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # calculate mean, var, and prior for each class\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors =  np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y==c]\n",
    "            self._mean[idx, :] = X_c.mean(axis=0)\n",
    "            self._var[idx, :] = X_c.var(axis=0)\n",
    "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = [self._predict(a) for a in x]\n",
    "        \n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # calculate posterior probability for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        # return class with highest posterior probability\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "            \n",
    "\n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(- (x-mean)**2 / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "nb = NaiveBayes()\n",
    "\n",
    "nb.fit(x_train, y_train)\n",
    "prediction = nb.predict(x_train)\n",
    "print(\"Naive Bayes classification accuracy on Training data\", accuracy(y_train, prediction))\n",
    "\n",
    "nb.fit(x_validation, y_validation)\n",
    "prediction = nb.predict(x_validation)\n",
    "print(\"Naive Bayes classification accuracyon Validation data\", accuracy(y_validation, prediction))\n",
    "\n",
    "prediction = nb.predict(x_test)\n",
    "print(\"Naive Bayes classification accuracy on Testing data\", accuracy(y_test, prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing columns with many unique values because the slow down computation\n",
    "df = pd.read_csv('data.csv',encoding='latin1')\n",
    "df.drop(labels=['gender:confidence'], axis=1, inplace = True)\n",
    "df.drop(labels=['fav_number'], axis=1, inplace = True)\n",
    "df.drop(labels=['profile_yn:confidence'], axis=1, inplace = True)\n",
    "df.drop(labels=['tweet_count'], axis=1, inplace = True)\n",
    "df.drop(labels=['Unnamed: 0'], axis=1, inplace = True)\n",
    "df.drop(labels=['_unit_id'], axis=1, inplace = True)\n",
    "df.drop(labels=['_last_judgment_at'], axis=1, inplace = True)\n",
    "df.drop(labels=['created'], axis=1, inplace = True)\n",
    "df.drop(labels=['description'], axis=1, inplace = True)\n",
    "df.drop(labels=['link_color'], axis=1, inplace = True)\n",
    "df.drop(labels=['name'], axis=1, inplace = True)\n",
    "df.drop(labels=['profileimage'], axis=1, inplace = True)\n",
    "df.drop(labels=['sidebar_color'], axis=1, inplace = True)\n",
    "df.drop(labels=['text'], axis=1, inplace = True)\n",
    "df.drop(labels=['tweet_coord'], axis=1, inplace = True)\n",
    "df.drop(labels=['tweet_created'], axis=1, inplace = True)\n",
    "df.drop(labels=['tweet_location'], axis=1, inplace = True)\n",
    "df.drop(labels=['user_timezone'], axis=1, inplace = True)\n",
    "df.drop(labels=['tweet_id'], axis=1, inplace = True)\n",
    "\n",
    "#move target column to the end\n",
    "df[\"gender\"].value_counts().sum()\n",
    "gender_column = df['gender']\n",
    "df.drop(labels=['gender'], axis=1, inplace = True)\n",
    "df.insert(11, 'gender', gender_column)\n",
    "\n",
    "#make retweet_count column to be discrete\n",
    "bins4 = [-math.inf,0, 33, 66, 99, 132, 165, 198, 231, 264, 297, 330, math.inf]\n",
    "df['retweet_count'] = pd.cut(df['retweet_count'], bins4)\n",
    "df['retweet_count'] = df['retweet_count'].astype(str)\n",
    "\n",
    "# remove brand\n",
    "df = df.drop(df[(df['gender'] == 'brand')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_df(df, train_size_ratio):\n",
    "    train_size = int(df.shape[0]*train_size_ratio)\n",
    "    df_train = df.head(train_size)\n",
    "\n",
    "    df_test_vidation = df.tail(df.shape[0]-train_size)\n",
    "\n",
    "    validation_size = int(0.2*df.shape[0])\n",
    "    df_validation = df.head(validation_size)\n",
    "\n",
    "    test_size = df.shape[0] -train_size-validation_size\n",
    "    df_test = df.tail(test_size)\n",
    "    \n",
    "    return df_train,df_validation,df_test\n",
    "\n",
    "def split(df,featureName, featureValue):\n",
    "    df_right = df[(df[featureName] == featureValue)]\n",
    "    df_left = df[(df[featureName] != featureValue)]\n",
    "    return df_right,df_left \n",
    "\n",
    "def main_Entropy(target):\n",
    "    targetArray = target['gender'].value_counts().values\n",
    "    total = targetArray.sum()\n",
    "    s = 0\n",
    "    if(total != 0):\n",
    "        for i in targetArray:\n",
    "            p1 = i/total\n",
    "            if (p1 == 0):\n",
    "                p2 = 0\n",
    "            else :\n",
    "                p2 = math.log2(p1)\n",
    "            s = s + p1*p2\n",
    "\n",
    "        s = -1*s\n",
    "    return s\n",
    "\n",
    "def subset_Entropy(df,FeatureName, subsetValue):\n",
    "    totalSubsetValue = df[(df[FeatureName] == subsetValue)].shape[0]\n",
    "    \n",
    "    if(totalSubsetValue != 0):\n",
    "        #Male probability\n",
    "        MP1 = df[(df[FeatureName] == subsetValue) & (df['gender'] == 'male')].shape[0]/totalSubsetValue\n",
    "\n",
    "        if (MP1 == 0):\n",
    "            MP2 = 0\n",
    "        else :\n",
    "            MP2 = math.log2(MP1)\n",
    "\n",
    "        #Female probability\n",
    "        MF1 = df[(df[FeatureName] == subsetValue) & (df['gender'] == 'female')].shape[0]/totalSubsetValue\n",
    "        if (MF1 == 0):\n",
    "            MF2 = 0\n",
    "        else :\n",
    "            MF2 = math.log2(MF1)\n",
    "\n",
    "        s = -1*(MP1*MP2 + MF1*MF2)\n",
    "    else:\n",
    "        s = 0\n",
    "    \n",
    "    return s\n",
    "\n",
    "def gain(df,FeatureName):\n",
    "    mainEntropy = main_Entropy(df)\n",
    "    \n",
    "    total = df.shape[0]\n",
    "    if(total != 0):\n",
    "        featureArray = df[[FeatureName]].values.flatten()\n",
    "\n",
    "        uniqueSubsetValues = np.unique(featureArray)\n",
    "\n",
    "        sss = 0\n",
    "\n",
    "        for i in uniqueSubsetValues:\n",
    "            ddd = np.count_nonzero(featureArray == i)\n",
    "            subsetEntropy = subset_Entropy(df, FeatureName, i)\n",
    "            sss = sss + ddd*subsetEntropy\n",
    "\n",
    "        gain =  mainEntropy - (1/total)*sss\n",
    "    else:\n",
    "        gain = 0\n",
    "    return gain\n",
    "\n",
    "def get_split_feature_value(df):\n",
    "    maxGain = -1000\n",
    "    features = df.columns\n",
    "    feature_size = df.shape[1]\n",
    "    for i in range(feature_size-1):\n",
    "        uniqueSubsetValues = np.unique(df[[features[i]]].values.flatten())\n",
    "        for value in uniqueSubsetValues:\n",
    "            df_right, df_left = split(df, features[i], value)\n",
    "            left_gain = gain(df_right,features[i])\n",
    "            right_gain = gain(df_left,features[i])\n",
    "            total_gain = left_gain+right_gain\n",
    "\n",
    "            if total_gain > maxGain:\n",
    "                maxGain = total_gain\n",
    "                split_feature = features[i]\n",
    "                split_value = value\n",
    "    return split_feature,split_value\n",
    "\n",
    "def decision_tree_algorithm(df, level=0, depth=4):\n",
    "    \n",
    "    uniqueSubsetValues = len(np.unique(df[['gender']].values.flatten()))\n",
    "    if (level == depth or uniqueSubsetValues == 1 or (df.shape[0] < 10)):\n",
    "        targets, counts_targets = np.unique(df[['gender']].values.flatten(), return_counts = True)\n",
    "        return targets[counts_targets.argmax()]\n",
    "    else:\n",
    "        level = level+1\n",
    "        \n",
    "        sf, sv = get_split_feature_value(df)\n",
    "        \n",
    "        df_right, df_left = split(df, sf, sv)\n",
    "        \n",
    "        question = \"{} == {}\".format(sf, sv)\n",
    "        \n",
    "        # instantiate sub-tree\n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        if(df_right.shape[0] != 0):\n",
    "            yes_answer = decision_tree_algorithm(df_right, level, depth)\n",
    "            \n",
    "        if(df_left.shape[0] != 0):\n",
    "            no_answer = decision_tree_algorithm(df_left, level, depth)\n",
    "            \n",
    "        if(df_right.shape[0] != 0 and df_left.shape[0] != 0):\n",
    "            if yes_answer == no_answer:\n",
    "                sub_tree = yes_answer\n",
    "            else:\n",
    "                sub_tree[question].append(yes_answer)\n",
    "                sub_tree[question].append(no_answer)\n",
    "                \n",
    "        elif(df_right.shape[0] != 0 and df_left.shape[0] == 0):\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            \n",
    "        elif(df_right.shape[0] == 0 and df_left.shape[0] != 0):\n",
    "            sub_tree[question].append(no_answer)\n",
    "    \n",
    "        else:\n",
    "            sub_tree[question].append(\"Error\")\n",
    "           \n",
    "        return sub_tree\n",
    "    \n",
    "    \n",
    "def predict(data_point,treee):\n",
    "    question1 = list(treee.keys())[0].split(\" \")\n",
    "    question = list(treee.keys())[0]\n",
    "    feature_name = question1[0]\n",
    "    value = question1[2]\n",
    "    \n",
    "    if data_point[feature_name] == value:\n",
    "        answer = treee[question][1]\n",
    "    else:\n",
    "        answer = treee[question][0]\n",
    "\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict(data_point,residual_tree)\n",
    "    \n",
    "    \n",
    "def accuracy_calculate(df, treee):\n",
    "    df[\"predicted\"] = df.apply(predict, args=(treee,), axis=1)\n",
    "    count = 0\n",
    "                        \n",
    "    confusion_matrix = [[0,1],\n",
    "                        [0,1]]\n",
    "    for ii in range(df.shape[0]):\n",
    "        if(df['gender'].values[ii] == df['predicted'].values[ii]):\n",
    "            count = count + 1\n",
    "            \n",
    "    accuracy = (count/df.shape[0])*100\n",
    "    \n",
    "    return accuracy, df['predicted'].values\n",
    "\n",
    "\n",
    "def hiddenLayer(real, pred, realV, predV):\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(real)):\n",
    "        if(real[i] == realV and pred[i] == predV):\n",
    "            count += 1;\n",
    "    return count;\n",
    "\n",
    "def confusionMatrix(realTarget, predictedTarget):\n",
    "    realTarget = np.array(realTarget)\n",
    "    predictedTarget = np.array(predictedTarget)\n",
    "    \n",
    "    \n",
    "    values = []\n",
    "    matrix = []\n",
    "    uniqueValues = np.array(np.unique(realTarget))\n",
    "    for i in range(len(uniqueValues)):\n",
    "        for j in range(len(uniqueValues)):\n",
    "            values.append(hiddenLayer(realTarget, predictedTarget, uniqueValues[i], uniqueValues[j]))\n",
    "        matrix.append(values)\n",
    "        values = []\n",
    "    m = pd.DataFrame({'pred female, male': matrix}, index =['Actual female', 'Actual male'])\n",
    "    return m\n",
    "\n",
    "\n",
    "df_train,df_validation, df_test = split_train_test_df(df, 0.6)\n",
    "tree = decision_tree_algorithm(df_train)\n",
    "acc_train, pred_train = accuracy_calculate(df_train, tree)\n",
    "acc_validation, pred_validation = accuracy_calculate(df_validation, tree)\n",
    "acc_test, pred_test = accuracy_calculate(df_test, tree)\n",
    "\n",
    "print(\"Decision Tree Algorithm classification accuracy on Training data\", acc_train)\n",
    "\n",
    "print(\"Decision Tree Algorithm classification accuracy on Validation data\", acc_validation)\n",
    "\n",
    "print(\"Decision Tree Algorithm classification accuracy on Testing data\", acc_test)\n",
    "print(\"Confusion Matrix:\")\n",
    "confusionMatrix(df_test['gender'].values, pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
